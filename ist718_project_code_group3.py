# -*- coding: utf-8 -*-
"""IST718_project_code_group3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13VlpQ36k30yb1GGe9xIWFOdNC0nyH39w
"""

import pandas as pd
# from google.colab import drive

# raw_data = '/content/drive/My Drive/ist718_data/data_group3/group3_data.csv'

# drive.mount('/content/drive')

raw_data_path = 'https://raw.githubusercontent.com/justinlim628/ist-718_spotify_project/master/data/data.csv'
raw_data = pd.read_csv(raw_data_path)


enable_grid_search = False

# Commented out IPython magic to ensure Python compatibility.
# import statements
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
import pandas as pd
import numpy as np
# %matplotlib inline
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.ml import feature
from pyspark.ml import classification
from pyspark.sql import functions as fn
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator, RegressionEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
import matplotlib.pyplot as plt
import seaborn as sns
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sqlContext = SQLContext(sc)
import os
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, PolynomialExpansion, VectorAssembler,StandardScaler
import pyspark.sql.functions as fn
from pyspark.ml.linalg import Vectors
from pyspark import keyword_only
from pyspark.ml import Transformer
from pyspark.ml.functions import vector_to_array
from pyspark.ml.param.shared import HasOutputCols, Param, Params
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
from pyspark.sql.functions import lit
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.types import FloatType
from pyspark.sql.types import IntegerType
from pyspark.sql.types import DateType
from pyspark.ml.stat import Correlation
from pyspark.sql.functions import isnan, isnull, when, count, col

"""### Data Loading / Preprocessing"""

spotify_2015_2020 = raw_data[raw_data['year']>=2015]
spotify_2015_2020.reset_index(drop=True, inplace=True)
spotify_2015_2020 = spotify_2015_2020.drop(['id'], axis=1)
spotify_df = spark.createDataFrame(spotify_2015_2020)

"""### Data Exploration"""

print(spotify_df.dtypes)

spotify_df.describe().show()

spotify_pd = spotify_df.toPandas()

print(spotify_pd.corr())

print(spotify_pd.cov())

spotify_df.select([count(when(isnull(c), c)).alias(c) for c in spotify_df.columns]).show()
spotify_df.select([count(when(isnan(c), c)).alias(c) for c in spotify_df.columns]).show()

p = sns.PairGrid(spotify_pd, vars=['acousticness','danceability','energy','instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo','valence','popularity','duration_ms'],hue='explicit')
p.map_diag(sns.kdeplot) # kernal density instead of scatter plot
p.map_offdiag(plt.scatter, s=15)

"""### Data Visualization"""

# Popularity distribution from the original data (1921 - 2020)
raw_data['popularity'].hist()
plt.xlabel('Popularity 1921-2020')
plt.ylabel('Frequency')
plt.title('Popularity Frequency from 1921 to 2020')
plt.show()
# Popularity distribution from the recent data (2015 - 2020)
spotify_2015_2020['popularity'].hist()
plt.xlabel('Popularity 2015-2020')
plt.ylabel('Frequency')
plt.title('Popularity Frequency from 2015 to 2020')
plt.show()
# isolate numeric columns only to analyze annual trend 
spotify_num = spotify_2015_2020.drop(['artists', 'explicit', 'mode', 'name', 'release_date', 'key'], axis=1)
year_trend = spotify_num.groupby('year').mean().drop(['duration_ms','tempo', 'popularity', 'loudness'], axis=1)

# plot yearly trend
plt.subplots(figsize=(7,7))
sns.lineplot(data=year_trend)
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.title('Trend in Songs from 2015 to 2020')
plt.show()
# plot popularity vs all other columns
y_col = 'popularity'
x_columns = spotify_num.columns.drop('popularity')

for x_col in x_columns:

    figure = plt.figure
    ax = plt.gca()
    ax.scatter(spotify_num[x_col], spotify_num[y_col])
    ax.set_xlabel(x_col)
    ax.set_ylabel(y_col)
    ax.set_title("{} vs {}".format(y_col, x_col))

    plt.legend()
    plt.show()

# plot histogram of all columns to observe the distribution in the columns
x_columns = spotify_num.columns

for x_col in x_columns:

    figure = plt.figure
    ax = plt.gca()
    ax.hist(spotify_num[x_col])
    ax.set_xlabel(x_col)
    ax.set_ylabel('Frequency')
    ax.set_title("{} Histogram".format(x_col))

    plt.legend()
    plt.show()

"""### Model

#### Linear Regression
"""

# split data
train, test, val = spotify_df_xformed.randomSplit([0.7,0.2, 0.1],seed = 11)

from pyspark.ml.feature import VectorAssembler


Assembler = VectorAssembler(inputCols = ['danceability', 'energy', 'loudness', 'explicit', 'key'],outputCol = 'features')
feature_engineering_pipe = Pipeline(stages = [Assembler])
model = feature_engineering_pipe.fit(spotify_df)
spotify_df_xformed = model.transform(spotify_df)

from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql import SQLContext
#from pyspark.mllib.linalg import Vectors
from pyspark.ml.linalg import Vectors, VectorUDT

# initial linear regression
linear_reg = LinearRegression(featuresCol="features",labelCol="popularity")
spotify_pipe = Pipeline(stages=[linear_reg])
model1 = spotify_pipe.fit(train)
linear_model = model1.stages[0]
print("Coefficients: " + str(linear_model.coefficients))
print("Intercept: " + str(linear_model.intercept))
model1_output = linear_model.transform(train)
trainingSummary = linear_model.summary
print("RMSE of Linear Regression Model: %f" % trainingSummary.rootMeanSquaredError)
#print("r2: %f" % trainingSummary.r2)
model1_evaluator = RegressionEvaluator(predictionCol="prediction",metricName="mse",labelCol='popularity')
print(model1_evaluator.metricName)
print('training MSE value: ',model1_evaluator.evaluate(model1_output))
print('testing MSE value: ',model1_evaluator.evaluate(linear_model.transform(test)))

model1_output.select("prediction", "popularity", "features").show(5)

if enable_grid_search:
  paramGrid = ParamGridBuilder()\
      .addGrid(linear_reg.regParam, [0, 0.1, 0.01]) \
      .addGrid(linear_reg.elasticNetParam, [0.0, 0.5, 1.0])\
      .build()
  cv1 = CrossValidator(estimator = spotify_pipe , estimatorParamMaps = paramGrid, evaluator = model1_evaluator, numFolds = 2)

  all_models = []
  for j in range(len(paramGrid)):
    cvmod1 = cv1.fit(train, paramGrid[j])
    all_models.append(cvmod1)

  mse = [RegressionEvaluator(labelCol='popularity', metricName='mse').evaluate(m.transform(test)) for m in all_models]
  predictions1 = cvmod1.transform(val)

  best_model_idx = np.argmin(mse)
  # find the best model
  best_model = all_models[best_model_idx]
  # locate the best model to find the best parameters
  print('Parameters of the best model: ', paramGrid[best_model_idx])


# key = predictions1.select("popularity", "prediction")
# for j in key.collect():
#    print(j)
  pass

# final linear regression: looks like default parameter settings are the best as the result from grid search suggest
linear_reg = LinearRegression(featuresCol="features",labelCol="popularity", regParam=0.0, elasticNetParam=0.0)
spotify_pipe = Pipeline(stages=[linear_reg])
model1 = spotify_pipe.fit(train)
linear_model = model1.stages[0]

# report coefficient and intercept
print("Coefficients: " + str(linear_model.coefficients))
print("Intercept: " + str(linear_model.intercept))

model1_output = linear_model.transform(train)
trainingSummary = linear_model.summary
print("RMSE of Linear Regression Model: %f" % trainingSummary.rootMeanSquaredError)
model1_evaluator = RegressionEvaluator(predictionCol="prediction",metricName="mse",labelCol='popularity')
print(model1_evaluator.metricName)
print('training MSE value: ',model1_evaluator.evaluate(model1_output))
print('testing MSE value: ',model1_evaluator.evaluate(linear_model.transform(test)))
lr_mse = model1_evaluator.evaluate(linear_model.transform(test))

"""#### Random Forest"""

# import for random forest
from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.evaluation import RegressionEvaluator
import numpy as np
from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.feature import VectorIndexer
from pyspark.sql.functions import isnan, isnull, when, count, col
from pyspark.ml.feature import StringIndexer
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, PolynomialExpansion, VectorAssembler,StandardScaler
import pyspark.sql.functions as fn
from pyspark.ml.linalg import Vectors
from pyspark import keyword_only
from pyspark.ml import Transformer
from pyspark.ml.functions import vector_to_array
from pyspark.ml import Pipeline
from pyspark.ml.param.shared import HasOutputCols, Param, Params
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
from pyspark.ml.evaluation import RegressionEvaluator

# initial random forest model
featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(spotify_df_xformed)
rf = RandomForestRegressor(featuresCol="indexedFeatures",labelCol="popularity")
pipeline = Pipeline(stages=[featureIndexer, rf])
model = pipeline.fit(train)
predictions = model.transform(test)
predictions.select("prediction", "popularity", "features").show(5)
evaluator = RegressionEvaluator(
    labelCol="popularity", predictionCol="prediction", metricName="mse")
mse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (MSE) on test data = %g" % mse)
rfModel = model.stages[1]
print(rfModel)

# tuning hyperparameters for random forest
if enable_grid_search:
  featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(spotify_df_xformed)
  rf = RandomForestRegressor(featuresCol="indexedFeatures",labelCol="popularity")
  pipeline = Pipeline(stages=[featureIndexer, rf])

  paramGrid = ParamGridBuilder() .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 10, stop = 50, num = 3)]) \
                  .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 25, num = 3)]) .build()

  evaluator = RegressionEvaluator(labelCol='popularity', predictionCol='prediction', metricName='rmse')
  cv = CrossValidator(estimator = pipeline , estimatorParamMaps = paramGrid, evaluator = evaluator, numFolds = 2)

  cvModel = cv.fit(train)
  predictions = cvModel.transform(val)
  bestPipe = cvModel.bestModel
  bestModel = bestPipe.stages[1]

  print('numTrees - ', bestModel.getNumTrees)
  print('maxDepth - ', bestModel.getOrDefault('maxDepth'))

  pass

# best random forest after hyperparameter tuning: numTrees=50, maxDepth=5 from grid search
indexer1 = VectorIndexer(inputCol = "features", outputCol = "featureindexes", maxCategories = 4).fit(spotify_df_xformed)
RF = RandomForestRegressor(featuresCol = 'features',labelCol = 'popularity', numTrees = 50, maxDepth = 5)
best_pipe = Pipeline(stages = [indexer1, RF])
impmodel = best_pipe.fit(train)
newpredictions = impmodel.transform(test)
# mse for training set
evaluator1 = RegressionEvaluator(labelCol='popularity', predictionCol='prediction', metricName="mse")
print("MSE for training: ", evaluator1.evaluate(impmodel.transform(train)))
# mse for test set
rf_mse = evaluator1.evaluate(newpredictions)
print("MSE for testing: ", rf_mse)

# report feature Importance
impmodel1 = impmodel.stages[-1]
c=impmodel1.featureImportances.toArray()
d=['danceability','energy','loudness', 'explicit', 'key']
feature_scoring = pd.DataFrame(list(zip(d,c )), columns = ['feature_name', 'importance_score']).sort_values(by = ['importance_score'], ascending = False)
rf_feature_importance=feature_scoring

display(rf_feature_importance)

# Model Comparison Table
compare_dict = {'model_name': ['Linear Regression', 'Random Forest'], 'mse': [lr_mse, rf_mse], 'rmse': [np.sqrt(lr_mse), np.sqrt(rf_mse)]}
compare_df = pd.DataFrame.from_dict(compare_dict)
print(compare_df)

"""From the comparison table above, it shows that the random forest performs better with lower mse score.

#### PCA
"""

# import packages for PCA and Kmeans
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext
from pyspark.sql import types
spark = SparkSession \
  .builder \
  .master("local[*]")\
  .config("spark.memory.fraction", 0.8) \
  .config("spark.executor.memory", "12g") \
  .config("spark.driver.memory", "12g")\
  .config("spark.memory.offHeap.enabled",'true')\
  .config("spark.memory.offHeap.size","12g")\
  .getOrCreate()
sc = spark.sparkContext
sqlContext = SQLContext(sc)
from pyspark.sql import functions as fn

import requests
from pyspark.ml import feature
from pyspark.ml import clustering
from pyspark.ml import Pipeline
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import isnan, isnull, when, count, col

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# first pca pipeline
spotify_pipe = Pipeline(stages=[feature.VectorAssembler(inputCols=['acousticness', 'danceability', 'energy', 'explicit', 
                                                                   'instrumentalness', 'key', 'liveness', 'loudness', 'mode',
                                                                   'speechiness', 'tempo', 'valence', 'year'],
                                                        outputCol= 'features'),
                                feature.StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='centered_features'),
                                feature.Normalizer(inputCol='centered_features', outputCol='norm_features'),
                                feature.PCA(k=13, inputCol='centered_features', outputCol='pca_score')
                                ])

spotify_model = spotify_pipe.fit(spotify_df)
spotify_pca = spotify_model.transform(spotify_df)
spotify_pca.show()

print(sum(spotify_model.stages[-1].explainedVariance))

# plot scree and cumulative sum of variance explained
plt.figure()
explained_var = spotify_model.stages[-1].explainedVariance
plt.plot(np.arange(1, len(explained_var)+1), explained_var)
plt.title("Scree Plot of Variance Explained")
plt.xlabel("Principal Component")
plt.ylabel("Proportion Variance Explained")
plt.show()

cum_sum = np.cumsum(explained_var)
plt.figure()
plt.plot(np.arange(1, len(explained_var)+1), cum_sum)
plt.title("Cumulative Sum of Variance Explained")
plt.xlabel("Cumulative Components")
plt.ylabel("Cumulative Sum of Variance Explained")
plt.show()
# final pca pipeline with k=10 this time
spotify_pipe = Pipeline(stages=[feature.VectorAssembler(inputCols=['acousticness', 'danceability', 'energy', 'explicit', 
                                                                   'instrumentalness', 'key', 'liveness', 'loudness', 'mode',
                                                                   'speechiness', 'tempo', 'valence', 'year'],
                                                        outputCol= 'features'),
                                feature.StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='centered_features'),
                                feature.Normalizer(inputCol='centered_features', outputCol='norm_features'),
                                feature.PCA(k=10, inputCol='centered_features', outputCol='pca_score')
                                ])

spotify_model = spotify_pipe.fit(spotify_df)
spotify_pca = spotify_model.transform(spotify_df)
print(sum(spotify_model.stages[-1].explainedVariance))
spotify_pca.show()

"""##### Recommendation"""

# define function that calcuate distance
def l2_dist(c1, c2):    
    return float(np.sqrt((c1 - c2).T.dot((c1 - c2))))

# define a function that output similar songs
def get_nearest_songs(song, num_nearest_song):
  l2_dist_udf = fn.udf(l2_dist, types.FloatType())
  song_df = spotify_pca.\
              where(spotify_pca.name == song).\
              select(fn.col("pca_score").alias('song_scores')).\
              join(spotify_pca).\
              withColumn('dist', l2_dist_udf('pca_score', 'song_scores')).\
              orderBy(fn.asc('dist')).\
              where('dist != 0').\
              select("name", 'artists','dist').\
              limit(num_nearest_song)
  
  return song_df.toPandas()

get_nearest_songs('Sad Forever', 5)

"""#### K-Means Clustering"""

# Commented out IPython magic to ensure Python compatibility.
# 2 minutes to run
# Code Source: https://runawayhorse001.github.io/LearningApacheSpark/clustering.html
# define a function that finds the best k
def optimal_k(df_in,k_min, k_max,num_runs):
    '''
    Determine optimal number of clusters by using Silhoutte Score Analysis.
    :param df_in: the input dataframe
    :param index_col: the name of the index column
    :param k_min: the train dataset
    :param k_min: the minmum number of the clusters
    :param k_max: the maxmum number of the clusters
    :param num_runs: the number of runs for each fixed clusters

    :return k: optimal number of the clusters
    :return silh_lst: Silhouette score
    :return r_table: the running results table

    :author: Wenqiang Feng
    :email:  von198@gmail.com
    '''

    silh_lst = []
    k_lst = np.arange(k_min, k_max+1)

    centers = pd.DataFrame()

    for k in k_lst:
        silh_val = []
        for run in np.arange(1, num_runs+1):

            # Trains a k-means model.
            kmeans = clustering.KMeans(k=k)
            model = kmeans.fit(df_in)

            # Make predictions
            predictions = model.transform(df_in)

            # Evaluate clustering by computing Silhouette score
            evaluator = ClusteringEvaluator()
            silhouette = evaluator.evaluate(predictions)
            silh_val.append(silhouette)

        silh_array=np.asanyarray(silh_val)
        silh_lst.append(silh_array.mean())

    silhouette = pd.DataFrame(list(zip(k_lst,silh_lst)),columns = ['k', 'silhouette'])

    return k_lst[np.argmax(silh_lst, axis=0)], silhouette

highest_score, silh_lst = optimal_k(spotify_pca,k_min=2, k_max=10,num_runs=5)

silhouette = spark.createDataFrame(silh_lst).toPandas()
print('k with the highest silhoutte score: ', highest_score, '\n')

# plot
silhouette.plot('k', 'silhouette')
plt.ylabel('silhouette score')
plt.title('Silhouette Score vs K')

# print silhouette score for each k
print(silhouette)

# final kmeans pipeline
spotify_kmeans_pipe = Pipeline(stages=[feature.VectorAssembler(inputCols=['acousticness', 'danceability', 'energy', 'explicit', 
                                                                   'instrumentalness', 'key', 'liveness', 'loudness', 'mode',
                                                                   'speechiness', 'tempo', 'valence', 'year'],
                                                                outputCol= 'features'),
                                       clustering.KMeans(k=6)])

spotify_kmeans_model = spotify_kmeans_pipe.fit(spotify_df)
spotify_kmeans = spotify_kmeans_model.transform(spotify_df)
spotify_kmeans.show()

# print silhouette score
ClusteringEvaluator().evaluate(spotify_kmeans)

# display frequency of each cluster
freq_cluster = spotify_kmeans.groupby('prediction').count()
freq_cluster.sort('prediction').show()